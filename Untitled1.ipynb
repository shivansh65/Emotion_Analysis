{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63dfa154-24a2-4176-b116-6fb03a5d22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0ca7cf9-7450-4ead-bc9f-3295d3282a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "def augment_audio(audio, sr):\n",
    "    noise = np.random.randn(len(audio))\n",
    "    audio_noise = audio + 0.005 * noise\n",
    "    audio_pitch = librosa.effects.pitch_shift(audio, sr=sr, n_steps=2)\n",
    "    return [audio, audio_noise, audio_pitch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71353310-350a-495c-9459-2e11273b6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path, sr=22050)\n",
    "    augmented_audios = augment_audio(audio, sample_rate)\n",
    "    features = []\n",
    "    for aug_audio in augmented_audios:\n",
    "        mfcc = librosa.feature.mfcc(y=aug_audio, sr=sample_rate, n_mfcc=40)\n",
    "        mfcc_scaled = np.mean(mfcc.T, axis=0)\n",
    "        features.append(mfcc_scaled)\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6ea7059-52af-4f4e-9e53-5542f0dac5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RAVDESS dataset\n",
    "data_path = \"./archive\"  # Update this path after downloading RAVDESS\n",
    "X, y = [], []\n",
    "emotions = {\"01\": \"neutral\", \"03\": \"happy\", \"04\": \"sad\", \"05\": \"angry\"}\n",
    "\n",
    "for root, _, files in os.walk(data_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".wav\"):\n",
    "            emotion_code = file.split(\"-\")[2]\n",
    "            if emotion_code in emotions:\n",
    "                file_path = os.path.join(root, file)\n",
    "                features = extract_features(file_path)\n",
    "                for feature in features:\n",
    "                    X.append(feature)\n",
    "                    y.append(emotions[emotion_code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8007e676-1974-4392-a4eb-31f29ee0dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "X = np.array(X)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "y_encoded = torch.tensor(y_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "29a8e6fb-d03c-406b-b92d-23c9d12b4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "29f9c356-6e09-4dd2-b75e-5d82bd854450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb134dc9-bf3c-4522-bb1e-5a4c350e4589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN-LSTM\n",
    "X_train = X_train.unsqueeze(2)\n",
    "X_test = X_test.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4534ee9-0ce7-4dcf-b828-dec2c6f86278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53f0cc33-187d-43db-a900-bd7ccac811cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hybrid CNN-LSTM model\n",
    "class EmotionDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EmotionDetectionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(128, 64, 5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.lstm1 = nn.LSTM(64, 128, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.lstm2 = nn.LSTM(128, 64, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x[:, -1, :])\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = EmotionDetectionModel(num_classes=len(emotions))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "62d924d3-2580-48df-a27d-bc79d69cdac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wb/3vrm6frx46j4_2yyzfjmh0940000gn/T/ipykernel_55276/3436060608.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float32)\n",
      "/var/folders/wb/3vrm6frx46j4_2yyzfjmh0940000gn/T/ipykernel_55276/3436060608.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_test = torch.tensor(X_test, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 1.356095283338339\n",
      "Epoch 2/100, Loss: 1.348395859841073\n",
      "Epoch 3/100, Loss: 1.261888832148939\n",
      "Epoch 4/100, Loss: 1.1643539832370116\n",
      "Epoch 5/100, Loss: 1.098966638640602\n",
      "Epoch 6/100, Loss: 1.032539421969121\n",
      "Epoch 7/100, Loss: 1.0084736778004335\n",
      "Epoch 8/100, Loss: 0.9575578658887656\n",
      "Epoch 9/100, Loss: 0.9114729301764233\n",
      "Epoch 10/100, Loss: 0.9024640835157716\n",
      "Epoch 11/100, Loss: 0.8618368223161981\n",
      "Epoch 12/100, Loss: 0.8560073806507753\n",
      "Epoch 13/100, Loss: 0.8365718745949244\n",
      "Epoch 14/100, Loss: 0.7925548854440746\n",
      "Epoch 15/100, Loss: 0.7740341720014515\n",
      "Epoch 16/100, Loss: 0.7487800088849398\n",
      "Epoch 17/100, Loss: 0.7371934445777742\n",
      "Epoch 18/100, Loss: 0.7188749095000843\n",
      "Epoch 19/100, Loss: 0.6900963550156886\n",
      "Epoch 20/100, Loss: 0.6680868805044948\n",
      "Epoch 21/100, Loss: 0.6416519656039701\n",
      "Epoch 22/100, Loss: 0.6142152913726202\n",
      "Epoch 23/100, Loss: 0.597919366442331\n",
      "Epoch 24/100, Loss: 0.5969571179092521\n",
      "Epoch 25/100, Loss: 0.5621621272351482\n",
      "Epoch 26/100, Loss: 0.5384008244122609\n",
      "Epoch 27/100, Loss: 0.526459271364873\n",
      "Epoch 28/100, Loss: 0.5135449557611258\n",
      "Epoch 29/100, Loss: 0.5041490256196202\n",
      "Epoch 30/100, Loss: 0.46440876238416917\n",
      "Epoch 31/100, Loss: 0.46125671459306583\n",
      "Epoch 32/100, Loss: 0.4352889592104619\n",
      "Epoch 33/100, Loss: 0.45183889287533147\n",
      "Epoch 34/100, Loss: 0.4057671713209388\n",
      "Epoch 35/100, Loss: 0.36555166771211245\n",
      "Epoch 36/100, Loss: 0.368994003770375\n",
      "Epoch 37/100, Loss: 0.3511282673271576\n",
      "Epoch 38/100, Loss: 0.3526633257175436\n",
      "Epoch 39/100, Loss: 0.3053008345979275\n",
      "Epoch 40/100, Loss: 0.2921383293474665\n",
      "Epoch 41/100, Loss: 0.3260658561150626\n",
      "Epoch 42/100, Loss: 0.2734420032120577\n",
      "Epoch 43/100, Loss: 0.3072181871474379\n",
      "Epoch 44/100, Loss: 0.2706536387305449\n",
      "Epoch 45/100, Loss: 0.25682511689639326\n",
      "Epoch 46/100, Loss: 0.26205091687417265\n",
      "Epoch 47/100, Loss: 0.29182813596902507\n",
      "Epoch 48/100, Loss: 0.22325119066356433\n",
      "Epoch 49/100, Loss: 0.22418250527122233\n",
      "Epoch 50/100, Loss: 0.20793556694937224\n",
      "Epoch 51/100, Loss: 0.24027447940984575\n",
      "Epoch 52/100, Loss: 0.21520105745680262\n",
      "Epoch 53/100, Loss: 0.2018939557273199\n",
      "Epoch 54/100, Loss: 0.18787460956759383\n",
      "Epoch 55/100, Loss: 0.23036653705236346\n",
      "Epoch 56/100, Loss: 0.2335925217681002\n",
      "Epoch 57/100, Loss: 0.19269403225124473\n",
      "Epoch 58/100, Loss: 0.17566582038629763\n",
      "Epoch 59/100, Loss: 0.1656410494395117\n",
      "Epoch 60/100, Loss: 0.1874186865579669\n",
      "Epoch 61/100, Loss: 0.21831562635627125\n",
      "Epoch 62/100, Loss: 0.15741783078059113\n",
      "Epoch 63/100, Loss: 0.16066967753121758\n",
      "Epoch 64/100, Loss: 0.18239188622130026\n",
      "Epoch 65/100, Loss: 0.17358742125959384\n",
      "Epoch 66/100, Loss: 0.1580786322529363\n",
      "Epoch 67/100, Loss: 0.15936769364346373\n",
      "Epoch 68/100, Loss: 0.17809896049375581\n",
      "Epoch 69/100, Loss: 0.17962749786490556\n",
      "Epoch 70/100, Loss: 0.15370673614342023\n",
      "Epoch 71/100, Loss: 0.14315049170912936\n",
      "Epoch 72/100, Loss: 0.11951316033702085\n",
      "Epoch 73/100, Loss: 0.1558920459148686\n",
      "Epoch 74/100, Loss: 0.14617217186432663\n",
      "Epoch 75/100, Loss: 0.1624904861212662\n",
      "Epoch 76/100, Loss: 0.1306992801791518\n",
      "Epoch 77/100, Loss: 0.14601748774178547\n",
      "Epoch 78/100, Loss: 0.14618636544166816\n",
      "Epoch 79/100, Loss: 0.11098864985316402\n",
      "Epoch 80/100, Loss: 0.1285266613306496\n",
      "Epoch 81/100, Loss: 0.16025811491884512\n",
      "Epoch 82/100, Loss: 0.10650474214373101\n",
      "Epoch 83/100, Loss: 0.09086929433656359\n",
      "Epoch 84/100, Loss: 0.10872980805648731\n",
      "Epoch 85/100, Loss: 0.14951080762513794\n",
      "Epoch 86/100, Loss: 0.15431003057222703\n",
      "Epoch 87/100, Loss: 0.14118265426985108\n",
      "Epoch 88/100, Loss: 0.11881266555997848\n",
      "Epoch 89/100, Loss: 0.07976542797981587\n",
      "Epoch 90/100, Loss: 0.0776041644247304\n",
      "Epoch 91/100, Loss: 0.0844347088062896\n",
      "Epoch 92/100, Loss: 0.09150702508839567\n",
      "Epoch 93/100, Loss: 0.11558086758405708\n",
      "Epoch 94/100, Loss: 0.0922281172755866\n",
      "Epoch 95/100, Loss: 0.10409599684160387\n",
      "Epoch 96/100, Loss: 0.12462568336562944\n",
      "Epoch 97/100, Loss: 0.09636953575905449\n",
      "Epoch 98/100, Loss: 0.12580744453502984\n",
      "Epoch 99/100, Loss: 0.13852790269152362\n",
      "Epoch 100/100, Loss: 0.10050601551382995\n",
      "Test Accuracy: 91.20%\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# Reshape for CNN-LSTM\n",
    "X_train = X_train.permute(0, 2, 1)  # Change shape from (batch_size, 40, 1) to (batch_size, 1, 40)\n",
    "X_test = X_test.permute(0, 2, 1)    # Change shape from (batch_size, 40, 1) to (batch_size, 1, 40)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Build hybrid CNN-LSTM model\n",
    "class EmotionDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(EmotionDetectionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, 5, padding=2)  # Input channels = 1\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(128, 64, 5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.lstm1 = nn.LSTM(64, 128, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.lstm2 = nn.LSTM(128, 64, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.permute(0, 2, 1)  # Reshape for LSTM: (batch_size, seq_len, channels)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x[:, -1, :])  # Use the last output of the LSTM\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = EmotionDetectionModel(num_classes=len(emotions))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluate\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64df902c-4f25-43de-917f-e26e214ca55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.20%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0095e988-4ad6-48ec-8e51-e6795a705c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and encoder\n",
    "torch.save(model.state_dict(), \"emotion_voice_model.pth\")\n",
    "np.save(\"label_encoder_classes.npy\", le.classes_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
